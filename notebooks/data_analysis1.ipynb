{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports, with path adjustment if necessary.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "directory_changed = False\n",
    "try:\n",
    "    from simulations.drosselschwab import simulate_drosselschwab\n",
    "except:\n",
    "    print(\"Adjusting path for imports...\")\n",
    "    os.chdir(\"..\")\n",
    "    directory_changed = True\n",
    "    from simulations.drosselschwab import simulate_drosselschwab\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import results\n",
    "import data\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scripts.parallel_sims import worker\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import powerlaw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if directory_changed:\n",
    "    base_dir = Path(\"/experiment_1\").resolve()\n",
    "else:\n",
    "    base_dir = Path(\"/experiment_1\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = Path.cwd()\n",
    "print(\"CWD:\", cwd)\n",
    "print(\"Working directory should be own_directory/experiment_1\")\n",
    "change_directory = input(\"If directory is not correct type desired path\")\n",
    "if change_directory != \"\":\n",
    "    os.chdir(change_directory)\n",
    "cwd = Path.cwd()\n",
    "print(\"New directory:\", cwd)\n",
    "\n",
    "files = list(cwd.rglob(\"*\"))\n",
    "print(\"Total files seen:\", len(files))\n",
    "\n",
    "\"\"\"\n",
    "Function that reads three dataframes.\n",
    "\"\"\"\n",
    "def find_three_datasets(idx):\n",
    "    files = list(Path.cwd().rglob(f\"perstep_param{idx}_*.csv\"))\n",
    "    print(\"Number of files found:\", len(files), end=\" \")\n",
    "    print(\"for index:\", idx)\n",
    "    if len(files) != 3:\n",
    "        raise ValueError(\"Less than three files found, check given index or directory\")\n",
    "    df1 = pd.read_csv(files[0])\n",
    "    df2 = pd.read_csv(files[1])\n",
    "    df3 = pd.read_csv(files[2])\n",
    "    return df1, df2, df3\n",
    "df1, df2, df3 = find_three_datasets(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is used to create an average of the three cells. The df contain a column for the fire sizes and the mean tree density. The standard average is taken for the mean tree densisty. The fire sizes are concatinated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function that takes three dataframes from per-step simulation results\n",
    "and averages them into a single dataframe.\n",
    "\n",
    "Each dataframe is expected to have columns:\n",
    "- \"fire_size\": a JSON-encoded list of fire sizes for that step\n",
    "- \"mean tree density\": a float representing the mean tree density at that step\n",
    "Returns a new dataframe with columns:\n",
    "- \"fire_sizes\": a flattened list of fire sizes from all three dataframes for each step\n",
    "\"\"\"\n",
    "\n",
    "def avg_df_3(df1, df2, df3):\n",
    "    dfs = [df1, df2, df3]\n",
    "    n_steps = min(len(df) for df in dfs)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        fire_lists = []\n",
    "        for df in dfs:\n",
    "            x = df.iloc[i][\"fire_size\"]\n",
    "            if isinstance(x, str):\n",
    "                x = json.loads(x) if x else []\n",
    "            fire_lists.append(x)\n",
    "\n",
    "        # flatten into a single list\n",
    "        fire_sizes_step = list(chain.from_iterable(fire_lists))\n",
    "\n",
    "        # average tree density (fix column name too!)\n",
    "        tree_density_step = np.mean(\n",
    "            [df.iloc[i][\"mean tree density\"] for df in dfs]\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"fire_sizes\": fire_sizes_step,\n",
    "            \"tree_density\": tree_density_step,\n",
    "        })\n",
    "\n",
    "\n",
    "    avg_df = pd.DataFrame(rows)\n",
    "    avg_df.head()\n",
    "    return avg_df\n",
    "\n",
    "def plot_average_tree_density(densities, ax=None, label=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.plot(densities, label=label)\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_ylabel(\"Average tree density\")\n",
    "    ax.set_title(\"Average tree density over time\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_fire_size_distribution(fire_sizes_col, label=None, nbins=25, density=True):\n",
    "    sizes = np.fromiter(chain.from_iterable(fire_sizes_col), dtype=np.int64)\n",
    "\n",
    "    smin, smax = sizes.min(), sizes.max()\n",
    "    bins = np.logspace(np.log10(smin), np.log10(smax), nbins + 1)\n",
    "\n",
    "    counts, edges = np.histogram(sizes, bins=bins)\n",
    "\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    if density:\n",
    "        widths = edges[1:] - edges[:-1]\n",
    "        pdf = counts / (counts.sum() * widths)\n",
    "        y = pdf\n",
    "        ylabel = \"Probability density\"\n",
    "    else:\n",
    "        pmf = counts / counts.sum()\n",
    "        y = pmf\n",
    "        ylabel = \"Probability\"\n",
    "\n",
    "    m = counts > 0\n",
    "    plt.loglog(centers[m], y[m], marker=\"o\", linestyle=\"-\", label=label)\n",
    "\n",
    "    plt.xlabel(\"Fire size\")\n",
    "    plt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_fire_sizes(series):\n",
    "    \"\"\"Flatten a pandas Series of lists/arrays into a 1D numpy int array.\"\"\"\n",
    "    arrs = []\n",
    "    for lst in series:\n",
    "        if isinstance(lst, (list, np.ndarray)) and len(lst) > 0:\n",
    "            arrs.append(np.asarray(lst, dtype=int).ravel())\n",
    "    if not arrs:\n",
    "        return np.array([], dtype=int)\n",
    "    x = np.concatenate(arrs)\n",
    "    x = x[np.isfinite(x)]\n",
    "    x = x[x > 0]\n",
    "    return x\n",
    "\n",
    "def empirical_ccdf(data_1d):\n",
    "    \"\"\"Empirical CCDF for 1D array (returns unique x and P(X>=x)).\"\"\"\n",
    "    data = np.asarray(data_1d)\n",
    "    data = data[np.isfinite(data)]\n",
    "    data = data[data > 0]\n",
    "    if data.size == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    data = np.sort(data)\n",
    "\n",
    "    x = np.unique(data)\n",
    "    # counts of >= x using searchsorted (fast)\n",
    "    idx = np.searchsorted(data, x, side=\"left\")\n",
    "    ccdf = (data.size - idx) / data.size\n",
    "    return x, ccdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different f values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected indexes and corresponding f values with fixed p=0.01\n",
    "indexes = [17, 18, 19, 20, 21]\n",
    "f_values = [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "avg_dfs = [avg_df_3(*find_three_datasets(i)) for i in indexes]\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    plot_fire_size_distribution(\n",
    "        df[\"fire_sizes\"],\n",
    "        label=f\"f = {f_values[i]}\",\n",
    "        nbins=25,\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "plt.title(\"Fire-size distribution (log–log) for p=0.01\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.axvline(4, color=\"k\", linestyle=\":\", linewidth=1)\n",
    "plt.axvline(300, color=\"k\", linestyle=\":\", linewidth=1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    plot_average_tree_density(\n",
    "        df[\"tree_density\"].to_numpy(),\n",
    "        ax=ax,\n",
    "        label=f\"f = {f_values[i]}\",\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different p values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [7, 19, 25, 44]\n",
    "indexes.reverse()\n",
    "p_values = [0.001, 0.01, 0.02, 0.2]\n",
    "p_values.reverse()\n",
    "print(\"Using p values:\", p_values)\n",
    "avg_dfs = [avg_df_3(*find_three_datasets(i)) for i in indexes]\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    plot_fire_size_distribution(\n",
    "        df[\"fire_sizes\"],\n",
    "        label=f\"p = {p_values[i]}\",\n",
    "        nbins=25,\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "plt.title(\"Fire-size distribution (log–log) for f=0.001\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    plot_average_tree_density(\n",
    "        df[\"tree_density\"].to_numpy(),\n",
    "        ax=ax,\n",
    "        label=f\"p = {p_values[i]}\",\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different f/p ratio's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [1, 3, 5, 7, 15, 17, 19, 21, 29, 31, 33, 35]\n",
    "f_over_p = [0.001, 0.01, 0.1, 1] * 3\n",
    "\n",
    "avg_dfs = [avg_df_3(*find_three_datasets(i)) for i in indexes]\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    plot_fire_size_distribution(\n",
    "        df[\"fire_sizes\"],\n",
    "        label=f\"f/p = {f_over_p[i]}\",\n",
    "        nbins=25,\n",
    "        density=True\n",
    "    )\n",
    "\n",
    "plt.title(\"Fire-size distribution (log–log) for different f/p ratios\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "avg_dfs = avg_dfs[4:8]\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    plot_average_tree_density(\n",
    "        df[\"tree_density\"].to_numpy(),\n",
    "        ax=ax,\n",
    "        label=f\"f/p = {f_over_p[i]}\",\n",
    "    )\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From visual to concrete data\n",
    "In this section we will use the powerlaw package to perform test to see if we are working with powerlaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "\n",
    "\n",
    "\n",
    "indexes = [29, 31, 33, 35]\n",
    "datasets = [\"f/p = 1\", \"f/p= 0.1 \" ,\"f/p = 0.01\", \"f/p = 0.001\"]\n",
    "\n",
    "avg_dfs = [avg_df_3(*find_three_datasets(i)) for i in indexes]\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    fire_sizes = flatten_fire_sizes(df[\"fire_sizes\"])\n",
    "    if fire_sizes.size < 50:\n",
    "        print(f\"f/p={f_over_p[i]}: not enough data after flattening\")\n",
    "        continue\n",
    "\n",
    "    # ---- Fit (on unbinned raw data) ----\n",
    "    fit = powerlaw.Fit(fire_sizes, discrete=True, verbose=False)\n",
    "\n",
    "    # ---- Model comparisons ----\n",
    "    R, p = fit.distribution_compare(\"power_law\", \"truncated_power_law\")\n",
    "    R2, p2 = fit.distribution_compare(\"truncated_power_law\", \"lognormal\")\n",
    "\n",
    "    print(f\"\\n=== {datasets[i]} ===\")\n",
    "    print(\"PL vs TPL: R=\", R, \"p=\", p)\n",
    "    print(\"TPL vs LN: R=\", R2, \"p=\", p2)\n",
    "    print(\"xmin:\", fit.xmin)\n",
    "    print(\"alpha (pure PL):\", fit.power_law.alpha)\n",
    "    print(\"alpha (truncated):\", fit.truncated_power_law.alpha)\n",
    "    print(\"lambda (cutoff rate):\", fit.truncated_power_law.Lambda)\n",
    "\n",
    "    # ---- Tail-only empirical CCDF (x >= xmin) ----\n",
    "    xmin = int(fit.xmin)\n",
    "    tail = fire_sizes[fire_sizes >= xmin]\n",
    "\n",
    "    x_emp, ccdf_emp = empirical_ccdf(tail)\n",
    "\n",
    "    # ---- Model CCDF on the same x-grid ----\n",
    "    # (use integer grid spanning the observed tail)\n",
    "    x_model = np.arange(int(x_emp.min()), int(x_emp.max()) + 1)\n",
    "    ccdf_tpl = fit.truncated_power_law.ccdf(x_model)\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.loglog(x_emp, ccdf_emp, \"o\", markersize=4, label=\"data (empirical CCDF, x>=xmin)\")\n",
    "    plt.loglog(x_model, ccdf_tpl, \"-\", linewidth=2, label=\"truncated power law (CCDF)\")\n",
    "\n",
    "    cutoff = 1.0 / fit.truncated_power_law.Lambda\n",
    "    plt.axvline(cutoff, ls=\"--\", color=\"k\", label=f\"cutoff ~ {cutoff:.1f}\")\n",
    "\n",
    "    plt.xlabel(\"Fire size\")\n",
    "    plt.ylabel(\"P(S ≥ s)\")\n",
    "    plt.title(f\"Fire-size CCDF ({datasets[i]}) | xmin={xmin}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the analysis strongly points to a truncated power law or lognormal (no significant difference). We see that the cutoff values increases as the f/p ratio grows. This gives us the impression that the scale-free characteristic of the fire-sizes get lost at high f/p, and so SOC behavior is absent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collapse\n",
    "\n",
    "The next cell will perform a data collapse.\n",
    "First we will try to fit $\\tau$, with the relation $P(s) \\sim s^{-\\tau}$ on the observed data.  $\\\\$\n",
    "We use the data before the cutoff on the plots above, which follow a power-law, see the dotted lines.  $\\\\$\n",
    "Now we find a cutoff value $f_c$ for each curve, we chose the 99% percentile. $\\\\$\n",
    "Then we rescale our data with the following value:\n",
    "$$P(s;f) = s^{-\\tau} F(\\frac{s}{f_c})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First try at data collapsing\n",
    "\"\"\"\n",
    "def sc_quantile(fire_sizes_col, q=0.99):\n",
    "    # cutoff ~ largest observed fire size\n",
    "    sizes = np.fromiter(chain.from_iterable(fire_sizes_col), dtype=np.int64)\n",
    "    sizes = sizes[sizes > 0]\n",
    "    return np.quantile(sizes, q)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Fire_size_histogram:\n",
    "input: column of fire sizes (lists)\n",
    "\n",
    "outpt:\n",
    "- centers: bin centers\n",
    "- y: probability density or mass\n",
    "\"\"\"\n",
    "\n",
    "indexes = [29, 31, 33, 35]\n",
    "f_over_p = [0.001, 0.01, 0.1, 1]\n",
    "avg_dfs = [avg_df_3(*find_three_datasets(i)) for i in indexes]\n",
    "\n",
    "def fire_size_histogram(fire_sizes_col, nbins=25, density=True):\n",
    "    sizes = np.fromiter(chain.from_iterable(fire_sizes_col), dtype=np.int64)\n",
    "    sizes = sizes[sizes > 0]\n",
    "\n",
    "    smin, smax = sizes.min(), sizes.max()\n",
    "    bins = np.logspace(np.log10(smin), np.log10(smax), nbins + 1)\n",
    "\n",
    "    counts, edges = np.histogram(sizes, bins=bins)\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    m = counts > 0\n",
    "    centers = centers[m]\n",
    "    counts = counts[m]\n",
    "\n",
    "    if density:\n",
    "        widths = edges[1:] - edges[:-1]\n",
    "        widths = widths[m]\n",
    "        y = counts / (counts.sum() * widths)\n",
    "    else:\n",
    "        y = counts / counts.sum()\n",
    "\n",
    "    return centers, y\n",
    "\n",
    "s, p = fire_size_histogram(avg_dfs[0][\"fire_sizes\"])\n",
    "\n",
    "#See chosen cutoff lines in previous plot\n",
    "mask = (s > 4) & (s < 300)\n",
    "\n",
    "log_s = np.log10(s[mask])\n",
    "log_p = np.log10(p[mask])\n",
    "\n",
    "slope, intercept = np.polyfit(log_s, log_p, 1)\n",
    "tau = -slope\n",
    "\n",
    "print(f\"Estimated tau ≈ {tau:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "curves = []\n",
    "\n",
    "for i, df in enumerate(avg_dfs):\n",
    "    s, p = fire_size_histogram(df[\"fire_sizes\"])\n",
    "\n",
    "    sc = sc_quantile(df[\"fire_sizes\"])\n",
    "\n",
    "    x = s / sc\n",
    "    y = (s ** tau) * p\n",
    "    curves.append((x, y))\n",
    "\n",
    "    plt.loglog(x, y, 'o', label=f\"f/p = {f_over_p[i]}\")\n",
    "\n",
    "plt.xlabel(r\"$s / s_c$\")\n",
    "plt.ylabel(r\"$s^{\\tau} P(s)$\")\n",
    "plt.title(\"Fire-size distribution data collapse for different f/p values\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_log_scatter(curves, x_min=None, x_max=None, n=80):\n",
    "    # auto overlap if not provided\n",
    "    if x_min is None or x_max is None:\n",
    "        xmin_list, xmax_list = [], []\n",
    "        for x, y in curves:\n",
    "            m = np.isfinite(x) & np.isfinite(y) & (x > 0) & (y > 0)\n",
    "            xmin_list.append(x[m].min())\n",
    "            xmax_list.append(x[m].max())\n",
    "        x_min = max(xmin_list)\n",
    "        x_max = min(xmax_list)\n",
    "\n",
    "    if not (x_min < x_max):\n",
    "        return np.nan\n",
    "\n",
    "    xg = np.logspace(np.log10(x_min), np.log10(x_max), n)\n",
    "\n",
    "    Ys = []\n",
    "    for x, y in curves:\n",
    "        m = np.isfinite(x) & np.isfinite(y) & (x > 0) & (y > 0)\n",
    "        x, y = x[m], y[m]\n",
    "        if len(x) < 2:\n",
    "            continue\n",
    "\n",
    "        order = np.argsort(x)\n",
    "        x, y = x[order], y[order]\n",
    "\n",
    "        m2 = (x >= x_min) & (x <= x_max)\n",
    "        x, y = x[m2], y[m2]\n",
    "        if len(x) < 2:\n",
    "            continue\n",
    "\n",
    "        yg = np.interp(np.log10(xg), np.log10(x), np.log10(y), left=np.nan, right=np.nan)\n",
    "        Ys.append(yg)\n",
    "\n",
    "    if len(Ys) < 2:\n",
    "        return np.nan\n",
    "    Y = np.vstack(Ys)\n",
    "    return np.nanmean(np.nanstd(Y, axis=0))\n",
    "\n",
    "scatter = mean_log_scatter(curves)\n",
    "print(\"Mean log-scatter:\", scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a log-scatter mean that is in the range of 0.2-0.3. From which we can conclude a succesfull data collaps. So the fire-size distribution obeys finite-size scaling. And the choice of f/p influences the cutoff, with a universal scaling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ast\n",
    "#os.chdir(\"../../experiment_1\")\n",
    "cwd = Path.cwd()\n",
    "print(\"CWD:\", cwd)\n",
    "\n",
    "files = list(cwd.rglob(\"*\"))\n",
    "print(\"Total files seen:\", len(files))\n",
    "\n",
    "def combine_cluster_distr(files, id_col=\"step\", list_col=\"cluster distr\"):\n",
    "    dfs = []\n",
    "    for f in files[:3]:\n",
    "        df = pd.read_csv(f, usecols=[id_col, list_col]).copy()\n",
    "        df[\"cluster distr\"] = df[\"cluster distr\"].map(\n",
    "        lambda s: np.fromstring(s.strip(\"[]\"), sep=\",\", dtype=int).tolist()\n",
    "            )\n",
    "        dfs.append(df)\n",
    "\n",
    "    # merge with unique suffixes per file\n",
    "    def merge_two(left, right, idx):\n",
    "        return left.merge(right, on=id_col, how=\"inner\", suffixes=(\"\", f\"__{idx}\"))\n",
    "\n",
    "    merged = dfs[0]\n",
    "    for idx, d in enumerate(dfs[1:], start=1):\n",
    "        merged = merge_two(merged, d, idx)\n",
    "\n",
    "    # collect all list columns (original + suffixed) and flatten row-wise\n",
    "    list_cols = [c for c in merged.columns if c.startswith(list_col)]\n",
    "    merged[list_col] = merged[list_cols].apply(\n",
    "        lambda r: list(chain.from_iterable(r)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return merged[[id_col, list_col]]\n",
    "\n",
    "\"\"\"\n",
    "Function that reads three dataframes.\n",
    "\"\"\"\n",
    "def find_clust_distr(idx):\n",
    "    files = list(Path.cwd().rglob(f\"perstep_param{idx}_*.csv\"))\n",
    "    print(\"Number of files found:\", len(files), end=\" \")\n",
    "    print(\"for index:\", idx)\n",
    "    if len(files) < 3:\n",
    "        raise ValueError(\"Less than three files found, check given index\")\n",
    "    return combine_cluster_distr(files)\n",
    "\n",
    "\n",
    "\n",
    "type(df1[\"cluster distr\"].iloc[0])\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def logbin_pdf(values, bins_per_decade=8):\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    values = values[np.isfinite(values)]\n",
    "    values = values[values > 0]\n",
    "    if values.size == 0:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    xmin, xmax = values.min(), values.max()\n",
    "    nbins = max(1, int(np.ceil((np.log10(xmax) - np.log10(xmin)) * bins_per_decade)))\n",
    "    edges = np.logspace(np.log10(xmin), np.log10(xmax), nbins + 1)\n",
    "\n",
    "    counts, edges = np.histogram(values, bins=edges)\n",
    "    widths = np.diff(edges)\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "\n",
    "    pdf = counts / (counts.sum() * widths)\n",
    "    m = counts > 0\n",
    "    return centers[m], pdf[m]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "indexes = [29, 31, 33, 35]\n",
    "f_over_p = [0.001, 0.01, 0.1, 1]\n",
    "for i, idx in enumerate(indexes):\n",
    "    df = find_clust_distr(idx)\n",
    "    # aggregate all steps into one long array\n",
    "    all_sizes = np.concatenate(df[\"cluster distr\"].values)\n",
    "\n",
    "    # (optional) drop size-1 clusters if you wan\n",
    "    all_sizes = all_sizes[all_sizes > 1]\n",
    "\n",
    "    x, y = logbin_pdf(all_sizes, bins_per_decade=8)\n",
    "\n",
    "    plt.plot(x, y, marker=\"o\", linewidth=2, label=f\"f/p = {f_over_p[i]}\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Cluster size\")\n",
    "    plt.ylabel(\"Probability density\")\n",
    "    plt.title(\"Cluster-size distribution (aggregated over steps)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
