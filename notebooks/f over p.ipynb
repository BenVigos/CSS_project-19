{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# In this notebook, we will explore the effect of the ratio of f to p on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulations.drosselschwab import simulate_drosselschwab\n",
    "import results\n",
    "import data\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from scripts.parallel_sims import worker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiments root under data/f_over_p and pick the next available experiment index\n",
    "base_dir = Path(\"../data/f_over_p\").resolve()\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "idx = 1\n",
    "while (base_dir / f\"experiment_{idx}\").exists():\n",
    "    idx += 1\n",
    "outdir = base_dir / f\"experiment_{idx}\"\n",
    "outdir.mkdir(parents=True, exist_ok=False)\n",
    "print(f\"Saving notebook run results to experiment directory: {outdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameter sweep: small demo grid. Replace with your actual sweep.\n",
    "param_list = []\n",
    "param_idx = 1\n",
    "L = 256\n",
    "steps = 1000\n",
    "\n",
    "runs_per_param = 5\n",
    "\n",
    "f_over_p_ratios = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "\n",
    "p_values = [0.1]\n",
    "f_values = [p_values[0]*f_over_p_ratio for f_over_p_ratio in f_over_p_ratios]\n",
    "\n",
    "for p in p_values:\n",
    "    for f in f_values:\n",
    "\n",
    "        for run_idx in range(runs_per_param):\n",
    "            param_list.append({'L': L, 'p': p, 'f': f, 'steps': steps, 'param_id': param_idx, 'run_id': run_idx})\n",
    "        param_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_workers = int(os.environ.get('MAX_WORKERS', multiprocessing.cpu_count()))\n",
    "print(f\"Running {len(param_list)} simulations with up to {max_workers} workers...\")\n",
    "\n",
    "results = []\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as exe:\n",
    "    # pass outdir as first argument to worker\n",
    "    futures = {exe.submit(worker, outdir, params): params for params in param_list}\n",
    "    for fut in as_completed(futures):\n",
    "        params = futures[fut]\n",
    "        try:\n",
    "            res = fut.result()\n",
    "            print(f\"Done: p={res['p']}, f={res['f']}, fires={res['num_fires']}, mean={res['mean_size']:.2f}, max={res['max_size']}\")\n",
    "            results.append(res)\n",
    "        except Exception as e:\n",
    "            print(f\"Error for params {params}: {e}\")\n",
    "\n",
    "# Write a summary CSV\n",
    "summary_file = outdir / f\"summary_{datetime.now().strftime('%Y%m%dT%H%M%SZ')}.csv\"\n",
    "keys = ['L', 'p', 'f', 'steps', 'param_id', 'run_id', 'num_fires', 'mean_size', 'max_size', 'remaining_trees', 'raw_file']\n",
    "with open(summary_file, 'w', newline='') as fh:\n",
    "    writer = csv.DictWriter(fh, keys)\n",
    "    writer.writeheader()\n",
    "    for r in results:\n",
    "        writer.writerow({k: r.get(k, '') for k in keys})\n",
    "\n",
    "print(f\"Summary written to {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New analysis cells: load per-step CSVs from the latest experiment and compute statistics ---\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# auto-detect latest experiment directory under data/f_over_p\n",
    "base_dir = Path(\"../data/f_over_p\").resolve()\n",
    "if not base_dir.exists():\n",
    "    raise FileNotFoundError(f\"Base data directory not found: {base_dir}\")\n",
    "exp_dirs = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith('experiment_')]\n",
    "if not exp_dirs:\n",
    "    raise FileNotFoundError(f\"No experiment directories found under {base_dir}\")\n",
    "def _exp_index(d):\n",
    "    try:\n",
    "        return int(d.name.split('_')[-1])\n",
    "    except Exception:\n",
    "        return -1\n",
    "exp_dirs_sorted = sorted(exp_dirs, key=_exp_index)\n",
    "EXP_DIR = exp_dirs_sorted[-1].resolve()\n",
    "print('Experiment dir:', EXP_DIR)\n",
    "\n",
    "# find perstep files created by the worker\n",
    "perstep_files = sorted(EXP_DIR.glob('perstep_param*_*.csv'))\n",
    "print(f'Found {len(perstep_files)} per-step files')\n",
    "\n",
    "# filename pattern to extract param_id and run_id\n",
    "pattern = re.compile(r'perstep_param(?P<param>\\d+)_.*_id(?P<run>\\d+)_')\n",
    "\n",
    "\n",
    "def load_perstep_file(fp):\n",
    "    records = []\n",
    "    with open(fp, newline='') as fh:\n",
    "        reader = csv.reader(fh)\n",
    "        try:\n",
    "            header = next(reader)\n",
    "        except StopIteration:\n",
    "            return records\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            # expected columns: step, fire_size (JSON), cluster distr (JSON), mean tree density\n",
    "            step = int(row[0])\n",
    "            fires = json.loads(row[1]) if row[1] else []\n",
    "            clusters = json.loads(row[2]) if row[2] else []\n",
    "            density = float(row[3]) if row[3] != '' else None\n",
    "            records.append({'step': step, 'fires': fires, 'clusters': clusters, 'density': density})\n",
    "    return records\n",
    "\n",
    "# organize runs by param_id\n",
    "runs_by_param = {}  # param_id -> list of runs\n",
    "for fp in perstep_files:\n",
    "    m = pattern.search(fp.name)\n",
    "    if not m:\n",
    "        print('Skipping unknown file pattern:', fp.name)\n",
    "        continue\n",
    "    pid = int(m.group('param'))\n",
    "    rid = int(m.group('run'))\n",
    "    recs = load_perstep_file(fp)\n",
    "    # aggregate series for convenience\n",
    "    fires_all = []\n",
    "    clusters_all = []\n",
    "    density_series = []\n",
    "    for r in recs:\n",
    "        fires_all.extend(r['fires'])\n",
    "        clusters_all.extend(r['clusters'])\n",
    "        density_series.append(r['density'])\n",
    "    runs_by_param.setdefault(pid, []).append({'run_id': rid, 'file': fp, 'records': recs, 'fires_all': fires_all, 'clusters_all': clusters_all, 'density_series': density_series})\n",
    "\n",
    "print('Loaded runs for param_ids:', sorted(runs_by_param.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Fire-size distributions for individual runs with the same param_id\n",
    "# Choose a param_id to inspect (if multiple present, this will loop over all)\n",
    "for pid in sorted(runs_by_param.keys()):\n",
    "    runs = runs_by_param[pid]\n",
    "    if not runs:\n",
    "        continue\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    # compute global min/max across runs for consistent bins\n",
    "    all_fs = np.concatenate([np.array(r['fires_all']) for r in runs if len(r['fires_all']) > 0]) if any(len(r['fires_all'])>0 for r in runs) else np.array([])\n",
    "    if all_fs.size == 0:\n",
    "        print(f'param_id {pid}: no fires recorded in any run')\n",
    "        continue\n",
    "    min_s = max(1, int(all_fs.min()))\n",
    "    max_s = int(all_fs.max())\n",
    "    bins = np.logspace(np.log10(min_s), np.log10(max_s), num=20)\n",
    "\n",
    "    for r in runs:\n",
    "        fs = np.array(r['fires_all'])\n",
    "        if fs.size == 0:\n",
    "            continue\n",
    "        hist, edges = np.histogram(fs, bins=bins, density=True)\n",
    "        centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "        mask = hist > 0\n",
    "        plt.loglog(centers[mask], hist[mask], marker='o', linestyle='-', label=f\"run {r['run_id']}\")\n",
    "\n",
    "    plt.title(f'Fire-size distributions for param_id {pid} (per-run)')\n",
    "    plt.xlabel('Fire size')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Fire-size distributions aggregated across runs for different param_ids\n",
    "# compute global bins across all param_ids\n",
    "all_fires_global = np.concatenate([np.concatenate([np.array(r['fires_all']) for r in runs_by_param[pid] if len(r['fires_all'])>0]) for pid in runs_by_param.keys() if any(len(r['fires_all'])>0 for r in runs_by_param[pid])]) if any(any(len(r['fires_all'])>0 for r in runs_by_param[pid]) for pid in runs_by_param.keys()) else np.array([])\n",
    "if all_fires_global.size > 0:\n",
    "    min_s = max(1, int(all_fires_global.min()))\n",
    "    max_s = int(all_fires_global.max())\n",
    "    bins = np.logspace(np.log10(min_s), np.log10(max_s), num=25)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for pid in sorted(runs_by_param.keys()):\n",
    "        runs = runs_by_param[pid]\n",
    "        # aggregate fires across runs for this param_id\n",
    "        agg = np.concatenate([np.array(r['fires_all']) for r in runs if len(r['fires_all']) > 0]) if any(len(r['fires_all'])>0 for r in runs) else np.array([])\n",
    "        if agg.size == 0:\n",
    "            continue\n",
    "        hist, edges = np.histogram(agg, bins=bins, density=True)\n",
    "        centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "        mask = hist > 0\n",
    "        plt.loglog(centers[mask], hist[mask], marker='o', linestyle='-', label=f'param {pid}')\n",
    "\n",
    "    plt.title('Fire-size distributions aggregated by param_id')\n",
    "    plt.xlabel('Fire size')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No fires in any run to plot aggregated distributions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Cluster-size distributions (aggregated across runs and steps) per param_id\n",
    "all_clusters_global = np.concatenate([np.concatenate([np.array(r['clusters_all']) for r in runs_by_param[pid] if len(r['clusters_all'])>0]) for pid in runs_by_param.keys() if any(len(r['clusters_all'])>0 for r in runs_by_param[pid])]) if any(any(len(r['clusters_all'])>0 for r in runs_by_param[pid]) for pid in runs_by_param.keys()) else np.array([])\n",
    "if all_clusters_global.size > 0:\n",
    "    min_c = max(1, int(all_clusters_global.min()))\n",
    "    max_c = int(all_clusters_global.max())\n",
    "    bins = np.logspace(np.log10(min_c), np.log10(max_c), num=25)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for pid in sorted(runs_by_param.keys()):\n",
    "        runs = runs_by_param[pid]\n",
    "        agg = np.concatenate([np.array(r['clusters_all']) for r in runs if len(r['clusters_all']) > 0]) if any(len(r['clusters_all'])>0 for r in runs) else np.array([])\n",
    "        if agg.size == 0:\n",
    "            continue\n",
    "        hist, edges = np.histogram(agg, bins=bins, density=True)\n",
    "        centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "        mask = hist > 0\n",
    "        plt.loglog(centers[mask], hist[mask], marker='o', linestyle='-', label=f'param {pid}')\n",
    "\n",
    "    plt.title('Cluster-size distributions aggregated by param_id')\n",
    "    plt.xlabel('Cluster size')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No clusters recorded in any run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Mean tree density time series per run and averaged per param_id\n",
    "for pid in sorted(runs_by_param.keys()):\n",
    "    runs = runs_by_param[pid]\n",
    "    if not runs:\n",
    "        continue\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # collect arrays of densities; pad shorter runs if needed\n",
    "    densities = [np.array(r['density_series'], dtype=float) for r in runs if len(r['density_series'])>0]\n",
    "    if not densities:\n",
    "        print(f'param {pid}: no density series available')\n",
    "        continue\n",
    "    maxlen = max(arr.size for arr in densities)\n",
    "    stacked = np.vstack([np.pad(arr, (0, maxlen - arr.size), constant_values=np.nan) for arr in densities])\n",
    "    # plot individual runs\n",
    "    for i, arr in enumerate(stacked):\n",
    "        plt.plot(np.arange(arr.size), arr, alpha=0.3, label=f'run {runs[i][\"run_id\"]}')\n",
    "    # mean and std across runs (ignore nan)\n",
    "    mean_series = np.nanmean(stacked, axis=0)\n",
    "    std_series = np.nanstd(stacked, axis=0)\n",
    "    x = np.arange(mean_series.size)\n",
    "    plt.plot(x, mean_series, color='k', linewidth=2, label='mean')\n",
    "    plt.fill_between(x, mean_series - std_series, mean_series + std_series, color='k', alpha=0.2, label='std')\n",
    "    plt.title(f'Mean tree density over time (param {pid})')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Mean tree density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of analysis cells\n",
    "print('Analysis complete')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
